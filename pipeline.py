import base64
from functools import partial
import random
import pathlib
import json
from typing import Any, Callable, List, Dict, Optional, Union
from matplotlib import pyplot as plt
from pydantic import BaseModel
import openai
import os
import dotenv
import shutil

from imagegen_flux import generate_image
# from imagegen_openai import generate_image

# loading environment variables
dotenv.load_dotenv()

# defining the path to the current directory
thisdir = pathlib.Path(__file__).parent.absolute()
client = openai.Client(api_key=os.getenv("OPEN_API_KEY"))

class Rating(BaseModel):
    summary: str
    explanation: str
    rating: int

"""
The auto_rate function uses OpenAI's GPT-4 model to generate a rating for an image based on a given prompt.
The function takes the following arguments:
- prompt: The prompt that was used to generate the image.
- image_path: The path to the image file.
- model: The GPT model to use for rating. Defaults to "gpt-4o-mini".
- rating_prompt_appendix: An optional string to append to the rating prompt. This can be used to provide additional context or instructions for the rating prompt.
The function returns a Rating object containing the summary, explanation, and rating generated by the GPT model.
"""
def auto_rate(prompt: str,
              image_path: pathlib.Path,
              model: str = "gpt-4o-mini",
              rating_prompt_appendix: Optional[str] = None) -> Rating:

    image_data = base64.b64encode(image_path.read_bytes()).decode("utf-8")

    rating_prompt = (
        f"Do you think this image generated by Generative AI from the prompt \"{prompt}\" "
        "is culturally appropriate and high quality? Be critical. Give an explanation for "
        "your answer and then rate it 1-5 (1 for very bad, 5 for great)."
    )
    if rating_prompt_appendix:
        rating_prompt += f" {rating_prompt_appendix}"

    completion = client.beta.chat.completions.parse(
        model=model,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": rating_prompt,
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_data}"
                        }
                    }
                ]
            }
        ],
        response_format=Rating
    )

    rating: Rating = completion.choices[0].message.parsed

    return rating


"""
The modify_prompt function uses OpenAI's GPT-4 model to modify a user input prompt to be more detailed and culturally appropriate.
The function takes the following arguments:
- prompt: The user input prompt to modify.
- examples: A list of example prompts and their modified versions, along with ratings and summaries.
The function returns the modified prompt generated by the GPT model.
If no examples are provided, the function uses a default example to demonstrate the modification process.
"""
def modify_prompt(prompt: str, examples: List[Dict[str, str]]) -> str:
    messages = [
        {
            "role": "system",
            "content": (
                "You are a prompt modifier that changes user input prompts so "
                "that they are detailed and culturally appropriate, avoiding "
                "stereotypes and assumptions. Use the provide summary according to the ratings provided to refine and improve the prompts"
            )
        }
    ]

    if not examples:
        examples = [
            {
                "original_prompt": "An asian family having dinner together",
                "modified_prompt": "A warm and inviting scene of an Asian family gathered around a dining table, sharing a meal together. The family consists of multiple generations, including grandparents, parents, and children, all engaging in conversation and laughter. The table is filled with a variety of traditional dishes, such as rice, vegetables, and steaming hot soup. The setting is a cozy, well-lit dining room with a homey atmosphere, decorated with cultural elements like wooden chopsticks, ceramic bowls, and a teapot. The expressions on their faces convey warmth, happiness, and togetherness.",
                "rating": 3.5,
                "summary": "The majority of reviewers found the image overall pleasant, but many noted that it reinforced stereotypes about what an Asian family looks like during dinner, particularly in terms of traditional clothing, decor, and food choices. While the warmth and togetherness were well captured, the depiction lacked diversity in how Asian families might actually dine, leading to an average rating of 3.5."
            }
        ]

    for example in examples:
        messages.extend([
            {
                "role": "user",
                "content": example['original_prompt']
            },
            {
                "role": "assistant",
                "content": example['modified_prompt']
            },
            {
                "role": "user",
                "content": f"Modified Prompt Rating: {example['rating']}. Rating Explanation: {example['summary']}"
            },
            {
                "role": "assistant",
                "content": f"Acknowledged"
            }
        ])
   
    messages.append({
        "role": "user",
        "content": f" Prompt: {prompt}\n"
    })

    # saving the inout and output prompts
    (thisdir / "messages.json").write_text(json.dumps(messages, indent=4, ensure_ascii=False))
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages
    )

    return response.choices[0].message.content


"""
The manual_rate function allows a user to manually rate an image based on a given prompt.
The function takes the following arguments:
- prompt: The prompt that was used to generate the image.
- image_path: The path to the image file.
The function prompts the user to provide an explanation for their rating and a rating score between 1 and 5.
The function returns a Rating object containing the user-provided explanation and rating.
"""
def manual_rate(prompt: str, image_path: pathlib.Path) -> Rating:
    print(f"Prompt: {prompt}")
    print(f"Image Path: {image_path}")
    image = plt.imread(image_path)
    plt.imshow(image)
    plt.axis("off")
    plt.show()

    while True:
        try:
            explanation = input(
                "Please provide an explanation for your rating: ")
            rating = input("Please rate the image (1 to 5): ")
            rating = int(rating)
            if rating < 1 or rating > 5:
                raise ValueError("Rating must be between 1 and 5.")
            break
        except ValueError as e:
            print(e)

    return Rating(explanation=explanation, rating=rating)


"""
The random_rate function generates a random rating for an image based on a given prompt.
The function takes the following arguments:
- prompt: The prompt that was used to generate the image.
- image_path: The path to the image file.
The function generates a random rating score between 1 and 5 and provides a default explanation.
The function returns a Rating object containing the random explanation and rating.
"""
def random_rate(prompt: str, image_path: pathlib.Path) -> Rating:
    rating = random.randint(1, 5)
    explanation = "Randomly Generated"
    return Rating(explanation=explanation, rating=rating)


RatingFunc = Callable[[str, pathlib.Path], Rating]


"""
The summarize_ratings function generates a short one-sentence summary of the feedback provided by multiple raters.
The function takes a list of Rating objects as input and combines the explanations into a single prompt for summarization.
The function uses OpenAI's GPT-4 model to generate a summary of the feedback.
The function returns the generated summary as a string.
""" 
def summarize_ratings(ratings: List[Rating]) -> str:
    explanations = "".join([r.explanation for r in ratings])

    summary_prompt = (
        "Give a very short one-sentence summary on the following feedback in a well structured manner "
        "highlighting the key points about the image quality and cultural relevance and why it got this much rating: "
        f"{explanations}"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "user",
                "content": summary_prompt
            }
        ]
    )

    return response.choices[0].message.content



"""
The rate_images function processes the ratings for a list of image items in the history.
The function takes the following arguments:
- history: A list of image items, each containing the prompt, image path, and ratings.
- num_users: The number of users providing ratings for each image.
- rating_funcs: A list of rating functions to use for each user.
The function processes the ratings for each image item in the history and calculates the average rating and summary.
The function updates the history items with the ratings, average rating, and summary.
"""
def rate_images(history: List[Dict[str, str]], num_users: int, rating_funcs: List[Callable[[str, pathlib.Path], Rating]]):
    for i, item in enumerate(history):
        if "ratings" not in item:
            ratings = [
                rating_funcs[j](item["modified_prompt"], pathlib.Path(item["image_path"]))
                for j in range(num_users)
            ]
            avg_rating = sum(r.rating for r in ratings) / len(ratings)
            summary = summarize_ratings(ratings)
            history[i].update({
                "ratings": [r.model_dump() for r in ratings],
                "rating": avg_rating,
                "summary": summary
            })

# Sanitizing strings
def sanitize_string(s: str) -> str:
    return s.replace(" ", "_").replace(",", "").replace(":", "").replace(";", "").replace(".", "").replace("?", "").replace("!", "")


# Defining the pipeline
def pipeline(prompts: List[str],
             iterations: int,
             images_per_prompt: int,
             best_n: int,
             num_users: int = 5,
             overwrite: bool = False,
             rating_func: Union[RatingFunc, List[RatingFunc]] = manual_rate):
    history_path = thisdir / "history.json"
    images_path = thisdir / "images"

    if not isinstance(rating_func, list):
        rating_func = [rating_func] * num_users
    
    if overwrite:
        if images_path.exists():
            shutil.rmtree(images_path)
        if history_path.exists():
            history_path.write_text("[]")

    history: List[Dict[str, Any]] = []
    if history_path.exists():
        history = json.loads(history_path.read_text())

    best_examples = sorted(history, key=lambda x: x["rating"], reverse=True)[:best_n]
    for prompt in prompts:
        for iteration in range(iterations):
            print(f"Processing Iteration {iteration + 1} for prompt: {prompt}")
            modified_prompts = [
                modify_prompt(prompt, best_examples)
                for _ in range(images_per_prompt)
            ]
            
            image_paths = []
            for i, mod_prompt in enumerate(modified_prompts):
                save_path = images_path / f"{sanitize_string(prompt)}/iteration_{iteration}/image_{i}.png"
                if save_path.exists():
                    print(f"Skipping image {save_path}")
                    image_paths.append(str(save_path))
                    continue

                generate_image(mod_prompt, save_path)

                history.append({
                    "iteration": iteration,
                    "original_prompt": prompt,
                    "modified_prompt": mod_prompt,
                    "image_path": str(save_path),

                })
                history_path.write_text(json.dumps(history, indent=4, ensure_ascii=False))

            rate_images(history, num_users=num_users, rating_funcs=rating_func)
            history_path.write_text(json.dumps(history, indent=4, ensure_ascii=False))
            best_examples = sorted(
                history,
                key=lambda x: x["rating"], reverse=True
            )[:best_n]

def main():
    prompt_path = thisdir / "prompts.json"
    prompts: List[str] = json.loads(prompt_path.read_text())

    auto_rating_funcs = [
        partial(
            auto_rate,
            model="gpt-4o-mini",
            rating_prompt_appendix=(
                "Culturally appropriate images should not contain any content that might "
                "perpetuate stereotypes or assumptions about a particular group of people. "
                "For example, images shouldn't depict people in traditional clothing while "
                "performing modern day-to-day activities."
            )
        ),
        partial(
            auto_rate,
            model="gpt-4o-mini",
            rating_prompt_appendix=(
                "Culturally appropriate images should not contain any content that might "
                "perpetuate stereotypes or assumptions about a particular group of people. "
            )
        ),
        partial(
            auto_rate,
            model="gpt-4o-mini",
            rating_prompt_appendix=(
                "Culturally appropriate images should pay homage to the target culture."
            )
        )
    ]

    pipeline(
        prompts=prompts[:1],
        iterations=5,
        images_per_prompt=2,
        best_n=3,
        num_users=len(auto_rating_funcs),
        overwrite=True,
        rating_func=auto_rating_funcs
    )


if __name__ == "__main__":
    main()


